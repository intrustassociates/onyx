# =============================================================================
# ONYX APLICAÇÃO - COOLIFY
# =============================================================================
# Stack de aplicação Onyx que faz deploy rápido.
# Conecta-se aos serviços de infraestrutura via rede compartilhada.
#
# DEPLOYMENT NO COOLIFY:
# 1. Certifique-se que o stack "onyx-infra" está rodando ANTES
# 2. Crie um novo projeto no Coolify chamado "onyx-app"
# 3. Use este arquivo docker-compose-coolify-app.yml
# 4. Configure as mesmas variáveis de ambiente
# 5. O Coolify vai expor automaticamente o web_server na porta 3000
#
# STORAGE S3:
# - Configure S3 externo obrigatoriamente (AWS, Cloudflare R2, Hetzner, etc)
# - Variáveis necessárias:
#   * S3_ENDPOINT_URL (ex: https://s3.amazonaws.com)
#   * S3_AWS_ACCESS_KEY_ID
#   * S3_AWS_SECRET_ACCESS_KEY
#   * S3_FILE_STORE_BUCKET_NAME
#
# IMPORTANTE:
# - Este stack conecta na rede "onyx-network" (externa, criada pela infra)
# - Os hostnames para conectar aos serviços de infra são:
#   * onyx-postgres (Postgres)
#   * onyx-vespa (Vespa)
#   * onyx-redis (Redis)
# =============================================================================

name: onyx-app

services:
  api_server:
    image: onyxdotapp/onyx-backend:${IMAGE_TAG:-latest}
    # COOLIFY: Se precisar buildar, descomente e ajuste o context
    # build:
    #   context: .
    #   dockerfile: backend/Dockerfile
    command: >
      /bin/sh -c "alembic upgrade head &&
      echo \"Starting Onyx Api Server\" &&
      uvicorn onyx.main:app --host 0.0.0.0 --port 8080"
    env_file:
      - path: .env
        required: false
    depends_on:
      - inference_model_server
    restart: unless-stopped
    environment:
      # Auth Settings
      - AUTH_TYPE=${AUTH_TYPE:-basic}
      # IMPORTANTE: Usar container_name dos serviços de infra
      - POSTGRES_HOST=onyx-postgres
      - VESPA_HOST=onyx-vespa
      - REDIS_HOST=onyx-redis
      - MODEL_SERVER_HOST=inference_model_server
      # S3 Configuration - Configure estas variáveis no Coolify
      - S3_ENDPOINT_URL=${S3_ENDPOINT_URL}
      - S3_AWS_ACCESS_KEY_ID=${S3_AWS_ACCESS_KEY_ID}
      - S3_AWS_SECRET_ACCESS_KEY=${S3_AWS_SECRET_ACCESS_KEY}
      - S3_FILE_STORE_BUCKET_NAME=${S3_FILE_STORE_BUCKET_NAME:-onyx-file-store-bucket}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - api_server_logs:/var/log/onyx
    networks:
      - onyx-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

  background:
    image: onyxdotapp/onyx-backend:${IMAGE_TAG:-latest}
    # COOLIFY: Se precisar buildar, descomente e ajuste o context
    # build:
    #   context: .
    #   dockerfile: backend/Dockerfile
    command: >
      /bin/sh -c "
      if [ -f /etc/ssl/certs/custom-ca.crt ]; then
        update-ca-certificates;
      fi &&
      /app/scripts/supervisord_entrypoint.sh"
    env_file:
      - path: .env
        required: false
    depends_on:
      - inference_model_server
      - indexing_model_server
    restart: unless-stopped
    environment:
      - USE_LIGHTWEIGHT_BACKGROUND_WORKER=${USE_LIGHTWEIGHT_BACKGROUND_WORKER:-true}
      # IMPORTANTE: Usar container_name dos serviços de infra
      - POSTGRES_HOST=onyx-postgres
      - VESPA_HOST=onyx-vespa
      - REDIS_HOST=onyx-redis
      - MODEL_SERVER_HOST=inference_model_server
      - INDEXING_MODEL_SERVER_HOST=indexing_model_server
      # S3 Configuration - Configure estas variáveis no Coolify
      - S3_ENDPOINT_URL=${S3_ENDPOINT_URL}
      - S3_AWS_ACCESS_KEY_ID=${S3_AWS_ACCESS_KEY_ID}
      - S3_AWS_SECRET_ACCESS_KEY=${S3_AWS_SECRET_ACCESS_KEY}
      - S3_FILE_STORE_BUCKET_NAME=${S3_FILE_STORE_BUCKET_NAME:-onyx-file-store-bucket}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - background_logs:/var/log/onyx
    networks:
      - onyx-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

  web_server:
    image: onyxdotapp/onyx-web-server:${IMAGE_TAG:-latest}
    # COOLIFY: Se precisar buildar, descomente e ajuste o context
    # build:
    #   context: .
    #   dockerfile: web/Dockerfile
    #   args:
    #     - NEXT_PUBLIC_POSITIVE_PREDEFINED_FEEDBACK_OPTIONS=${NEXT_PUBLIC_POSITIVE_PREDEFINED_FEEDBACK_OPTIONS:-}
    #     - NEXT_PUBLIC_NEGATIVE_PREDEFINED_FEEDBACK_OPTIONS=${NEXT_PUBLIC_NEGATIVE_PREDEFINED_FEEDBACK_OPTIONS:-}
    #     - NEXT_PUBLIC_DISABLE_LOGOUT=${NEXT_PUBLIC_DISABLE_LOGOUT:-}
    #     - NEXT_PUBLIC_DEFAULT_SIDEBAR_OPEN=${NEXT_PUBLIC_DEFAULT_SIDEBAR_OPEN:-}
    #     - NEXT_PUBLIC_FORGOT_PASSWORD_ENABLED=${NEXT_PUBLIC_FORGOT_PASSWORD_ENABLED:-}
    #     - NEXT_PUBLIC_THEME=${NEXT_PUBLIC_THEME:-}
    #     - NEXT_PUBLIC_DO_NOT_USE_TOGGLE_OFF_DANSWER_POWERED=${NEXT_PUBLIC_DO_NOT_USE_TOGGLE_OFF_DANSWER_POWERED:-false}
    #     - NODE_OPTIONS=${NODE_OPTIONS:-"--max-old-space-size=4096"}
    env_file:
      - path: .env
        required: false
    depends_on:
      api_server:
        condition: service_healthy
    restart: unless-stopped
    # COOLIFY: Porta exposta para o proxy reverso do Coolify
    ports:
      - "3000:3000"
    environment:
      - INTERNAL_URL=${INTERNAL_URL:-http://api_server:8080}
    networks:
      - onyx-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

  inference_model_server:
    image: onyxdotapp/onyx-model-server:${IMAGE_TAG:-latest}
    # COOLIFY: Se precisar buildar, descomente e ajuste o context
    # build:
    #   context: .
    #   dockerfile: backend/Dockerfile.model_server
    # GPU Support: Uncomment the following lines to enable GPU support
    # Requires nvidia-container-toolkit to be installed on the host
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    command: >
      /bin/sh -c "if [ \"${DISABLE_MODEL_SERVER:-false}\" = \"True\" ]; then
        echo 'Skipping service...';
        exit 0;
      else
        exec uvicorn model_server.main:app --host 0.0.0.0 --port 9000;
      fi"
    env_file:
      - path: .env
        required: false
    restart: unless-stopped
    volumes:
      # Not necessary, this is just to reduce download time during startup
      - model_cache_huggingface:/app/.cache/huggingface/
      # Optional, only for debugging purposes
      - inference_model_server_logs:/var/log/onyx
    networks:
      - onyx-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

  indexing_model_server:
    image: onyxdotapp/onyx-model-server:${IMAGE_TAG:-latest}
    # COOLIFY: Se precisar buildar, descomente e ajuste o context
    # build:
    #   context: .
    #   dockerfile: backend/Dockerfile.model_server
    # GPU Support: Uncomment the following lines to enable GPU support
    # Requires nvidia-container-toolkit to be installed on the host
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    command: >
      /bin/sh -c "if [ \"${DISABLE_MODEL_SERVER:-false}\" = \"True\" ]; then
        echo 'Skipping service...';
        exit 0;
      else
        exec uvicorn model_server.main:app --host 0.0.0.0 --port 9000;
      fi"
    env_file:
      - path: .env
        required: false
    restart: unless-stopped
    environment:
      - INDEXING_ONLY=True
    volumes:
      # Not necessary, this is just to reduce download time during startup
      - indexing_huggingface_model_cache:/app/.cache/huggingface/
      # Optional, only for debugging purposes
      - indexing_model_server_logs:/var/log/onyx
    networks:
      - onyx-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "6"

volumes:
  # Caches to prevent re-downloading models, not strictly necessary
  model_cache_huggingface:
  indexing_huggingface_model_cache:
  # Logs preserved across container restarts
  api_server_logs:
  background_logs:
  inference_model_server_logs:
  indexing_model_server_logs:

networks:
  onyx-network:
    external: true
    name: onyx-network

